---
title: "Classification Project"
author: "Uthpala Perera"
date: "Thursday, November 12, 2015"
output: html_document
---

This paper investigates what would be a good classification model for human resouce activity measurements obtained from http://groupware.les.inf.puc-rio.br/har . The measurements are obtained from six participents performing a weight lifting exercise in 5 deliberate ways which we seek to  classify/predict.



```{r,echo=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)
library(caret)

```


We begin by reading the training and test datasets

```{r,cache=TRUE}

data <- read.csv("c:/temp/coursework-mlearning/pml-training.csv")
final <- read.csv("c:/temp/coursework-mlearning/pml-testing.csv")

```

##Data Exploration and Cleaning

A quick, first look, at the data set with R functions such as summary and View shows a lot of variables with many missing values. Those with over 90% missing values will be removed. Variables such as names, time stamps and row labels will be removed from the model as including them will be misleading for the decision.


```{r}
#get the list of columns with over 90% blanks
count.blanks <- function(x){sum(x== "")/length(x)}
blanks <- apply(data,2, count.blanks)
exnames <- names(data)[blanks > .90]
exlist <- exnames[!is.na(exnames)]
#get the list of columns with over 90% NAs
count.nas <- function(x){sum(is.na(x))/length(x)}
nas <- apply(data,2, count.nas)
nanames <- names(data)[nas > .90]
nalist <- nanames[!is.na(nanames)]
rmlist <- append(exlist, nalist)
tmlist <- grep("time", names(data),value=TRUE)
rmlist <- append(rmlist, tmlist)
rmlist[length(rmlist)+1] <- "X"
rmlist[length(rmlist)+1] <- "user_name"
rmlist[length(rmlist)+1] <- "num_window"
rmlist[length(rmlist)+1] <- "new_window"
newdata <- data[,!colnames(data)%in%rmlist]
newfinal <- final[,!colnames(data)%in%rmlist]
```


The following will  slice the cleaned training data into a training and validation set.


```{r}
library(caret)
set.seed(9123)
inTrain <- createDataPartition(y=newdata$classe, p=0.9, list=FALSE)
training <- newdata[inTrain,]
validation <- newdata[-inTrain,]
```

We shall explore the training set with a Random Forest prediction model as they are known to perform well with a large number of variables as in this case.


```{r,cache=TRUE,message=FALSE}
library(doParallel)
registerDoParallel()
set.seed(1224)
testmod <- train(classe ~ ., training, method="rf")

```

Let us now find the 20 most important predictors

```{r,message=FALSE}
vimp <-varImp(testmod)
plot(vimp,top=20)
```


```{r,echo=FALSE}
tst <- vimp$importance
tst <- add_rownames(tst)
test <- arrange(tst,desc(Overall))
cols <- test$rowname[1:20]
cols[21] <- "classe"
fintrain <- training[,cols]  

```

The initial model will be refined with just these 20 most important predictors


```{r,cache=TRUE}
set.seed(1334)
fit <- train(classe~., fintrain, method = "rf")

```

## Estimate the "of out of sample error rate"  using the validation set. 

```{r}
pred <- predict(fit, validation)
predright = pred == validation$classe
(length(pred)- sum(predright))/length(pred)
```

Let us look at the following two plots of predictions for the validation-set where the missclassifications are. They are explained by the  right top most cluster where all five classes are intermixed with few clear boundaries . 

```{r}
qplot(roll_belt,yaw_belt,data=validation,color= predright,main="Validation Predictions")
qplot(roll_belt,yaw_belt,data=validation,color=classe)
```

##Conclusion

The estimated out of sample error rate is 0.8% for a classification model of Random Forests with 20 predictors. We shall choose this as a suitable model for predicting the test set.


The prediction for the test set is as follows:

```{r}
predict(fit, newfinal)

```




